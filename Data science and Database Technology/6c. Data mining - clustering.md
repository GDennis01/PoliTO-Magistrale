## Cluster Analysis
Finding groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups. Usually performed for **understanding** or **summarization**

## Types of Clusters

### Partitional clustering
Data is divided into non overlapped subsets(clusters)
### Hierarchical clustering
A set of nested clusters organized as a hierarchical tree.
Cluster sets can be visualized as a dendogram, a specific kind of tree.
![[dendograms.png]]

### Others type
#### Exclusive versus non-exclusive
In non-exclusive, points belong to multiple clusters.
#### Fuzzy versus non fuzzy
In fuzzy clustering, a point belongs to every cluster with a weight from 0 to 1. 
Weights must sum to 1.
#### Well-separated clusters
A cluster is a set of points such that any point in a cluster is closer (or more similar) to every other point in the cluster than to any point not in the cluster.
![[well-separated-cluster.png]]
*well-separated cluster*

#### Center-based clusters
A cluster is a set of objects such that an object in a cluster is closer (more similar) to the “center” of a cluster, than to the center of any other cluster.
The center can be either a **centroid**(average of all points) or a **medoid**(most representative point)

#### Density-based clusters
In this case, a cluster is a dense area of objecsts separated by a low-area density.
![[density_based.png]]

## Clustering Algorithm

### K-means clustering
It uses a partitional clustering approach.
Each cluster is associated with a *centeroid*
Each point is assigned to the cluster with the closest *centroid*
At the beginning, a number of cluster **K** must be specified.
![[k-means_clustering.png]]
Step 1 is often performed randomly.
It converges after  few iterations. It can also stops until a certain condition is met.

#### Evaluating a K-means cluster set
The most common measure is *SSE- Sum of Squared Errors:*
![[sse_formula.png]]
where $m_i$ is the *centroid*(or representative point) of the cluster and $x$ is a point in the $C_i$ cluster.
The higher the SSE, the lower the cohesion inside a cluster

#### Solutions to initial centroid
##### Multiple runs
The clustering algorithm is run multiple times. A bruteforce approach basically
##### Using hierarchical clustering
We can sample centroids from a hierarchical cluster sets.
##### Knee approach
The idea behind this approach is to find a *knee* in an Elbow graph.
A knee is a location in the graph where the tend changes.
That location is where the best point of **k** resides.
![[elbow_graph.png]]
*In this case, around 5-6 we can notice a knee. Thus optimal K should be around that range*
##### Post-processing
We can post-process a solution produced by an algorithm in an earlier run.
We can split *loose* clusters, clusters that have high **SSE** or merge clusters that have low **SSE**. We can also eliminate small clusters that represents outliers.


### Bisecting K-means
A variant of the basic K-means algorithm where each time we focus only a specific portion of the data collection.
Each time, we run k-means with K=2 on the collection with the highest **SSE.**
**Bisecting K-means** can also be used to produced **centroids** for a *basic K-means*
algorithm.
![[bisecting_kmeans.png]]
*Pseudocode of Bisecting K-means*

#### Limitations of K-means
They are susceptible to outliers and have problems with clusters with different shapes/size.
To overcome this, we may increase K to obtain more clusters and then merge them together.

### Hierarchical Clustering
Unlike the algorithms described above, here no assumptions are made on the number of the expected clusters.
Two different approaches:
- **top-down:** from a single cluster containing all data to multiple clusters. It's a divise approach.
- **bottom-up:** from clusters of single points up to a single cluster. It's an **agglomerative** approach and it's the most used

#### Agglomerative clustering algorithm
Firstly, a **proximity matrix** is built containing all the distances/similarities between each possible pair of clusters.
Then it merges the two closest clusters and lastly the matrix is updated. 
This last step is performed until a single cluster remains.
![[pseudo_agglo.png]]
*Pseudocode*

The key operation is the computation of the **proximity matrix.**
Thus, it's also crucial the definition of the inter-cluster similarity.
Multiple options to choose from:
- **MIN(or single-link):** minimum  distance between the two closest points of the two clusters.
  It's sensitive to outliers and noise. Can handle non-elliptical shapes
  
- **MAX(or complete-linkage):** maximum distance between the two most distant points of the two clusters
  Less susceptible to noise and outliers. Tends to break large clusters and is biased towards globular clusters.

- **Group average:** average distance between two clusters
![[group_average.png]]
Less susceptible to noise and outliers.

- **Distance between centroids:** the name is self-explanatory

- Other **objective functions** like **SSE(Ward's method)**: In case of **SSE**, we can use this algorithm to initialize **K-means**

#### Time and space complexity
Space complexity of $O(N^2)$  since it uses  the proximity matrix
Time complexity of $O(N^3)$ in many cases due to $N$ steps and each step updates a matrix($N^2$)

### Density-based clustering - DBSCAN
#### Definitions
The density is specified by a number of points withing a specified radius(*Eps*).
A point is said to be **core point** if it has more than a specified number of points(MinPts) withing *Eps*.
A point is said to be a **border point** if it has fewer than MinPts withing *Eps* but it's in the neighbourhood of a **core point**.
Lastly, a point is said to be a **noise point** if it's neither a border or a core point.

#### Algorithm
The core point is used to initialize a cluster.
This area contains border points and other core points.
The area gets expanded further based on other core points.
Firstly, we eliminate noise points, then
![[pseudo_dbscan.png]]
The identification of core points,border points and noise points is performed as the very first step of the DBSCAN algorithm.
#### Performances
It works well and it's robust to noises as it can handle clusters of differnt shapes/sizes.
However, in a high-dimensional data setting with varying densities, DBSCAN doesn't perform too well

#### Determining EPS and MinPts
![[eps_minpts.png]]
On the Y-axis we have the radius(EPS) and on the X-axis the number of elements inside said radius.
Let's say we have k=minPts=4(as in the figure), we have that with radius of 10, we have about 2750 points.
As the EPS increases from 10 onwards, the trend goes up exponentially as it includes outliers.

## Cluster Validation
Several aspects to take into consideration:
1. Determining **clustering tendency**: that is,  distinguishing whether a non-random structure exist
2. Comparing the result of a cluster analysis to *externally* known results(e.g. given class labels)
3. Comparing two different clusters to see which is better
4. Evaluating how well the results of a cluster analysis fit the data without reference to external information.

### Measures
We can use different indexes, based on the aspect we are trying to measure.
#### Internal measures
##### Cluster cohesion 
Used to evaluate if objects in same cluster are similar
Computed through **SSE:**
![[sse_cohesion.png]]
##### Cluster separation
Used to evaluate if objects placed in different clusters are different.
Computed by measuring the cluster sum of squares:  ![[sse_separation.png]]
Both indexes are computed through the means of a representative point: the **centroid**.
These measures can also be computed by using a *proximity* graph
##### Silhouette index
It's computed for each points in the data collection and considers both *cohesion* and *separation*.
The following formula computes said index:
![[silhouette.png]]
where:
- **a(i)** is the average *dissimilarity* for a given object *i* with all other objects in the same cluster
- **b(i)** is the minimun of all the average dissimilarities for a given object **i** to any other cluster's objects.
It gives a value in a range of $[-1,1]$, the closer to 1, the better.
The average of $s(i)$ over all data of a cluster measures how tightly grouped all the data in the cluster are.
The average of $s(i)$ over all data of the dataset measures how appropriately the data has been clustered.

#### External Measures
##### Entropy
![[entropy.png]]
##### Purity
![[purity.png]]
#### Relative measures
##### Rand index
The idea behind this is that two objects belonging to the same cluster should also belong to the same class.
![[rand_index.png]]