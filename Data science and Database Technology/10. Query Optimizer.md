### Intro
It's in charge of selecting and generating an efficient strategy for query execution(**query execution plan**): it evaluates different alternatives, exploit statistics stored in the system catalog, different strategies and dynamically adapt to changes in the data distribution.
It also guarantees the **data independency property**, that is, the way a SQL query is written does not affect the way in which it is implemented and viceversa, a physical reorganization of data does not require rewriting SQL queries.
There are several steps to the overall optimization.

#### Overview of Lexical, Syntactic and Semantic analysis
In this step, the **optimizer** checks for any **lexical**,**syntactic** or **semantic** errors.
Once the analysis is done, it translates the query into an internal procedural representation in *extended* **relational algebra**. This representation is chosen since it is procedural (order based)

#### Overview of Algebraic optimization
A set of transformations that are considered to be always beneficial, such as moving the selection before the join.
This step is usually independent of the data distribuction.
In this step, a **query tree** in canonical form is outputd.

#### Overview of Cost based optimization
Selection of the best execution plan by evaluating the **execution cost.**
It decides the selection of:
- **access methods** for each table
- **algorithm** for each relational operator.
It takes decisions based on cost model for access methods and algorithms.
An **executable** is generated at the end of this step.

#### Overview of Execution modes
Two different modes
##### Compile and go
Compilation and immediate execution of the statement, without storing it. No dependencies are needed
##### Compile and store
Compilation and storing it in the database together with its dependencies.
It's executen then **on demand.**
Whenever the data structure changes, a recompilation is needed

### Algebraic Optimization
It is based on equivalence transformations, that is, two relational expressions are equivalent if they produce the same result.
The main objective is to reduce the size of the intermediate result to be stored in memory 
There are several transformations

#### Atomization of selection
A conjuction(AND) of selection can be transformed in nested selections.
![[atomization_selection.png]]
<sup><sub>σ=selection | F1,F2=conditions | E=relation table</sub></sup>
Basically it is anticipating a condition on E, then applying the other one
#### Cascading projections
Almost the same concept of the **atomization of selection**
![[cascading_projects.png]]
<sup><sub>π=projection | X,Y=subset of attributes </sub></sup>
Get the X attributes starting from a set that includes also Y
#### Anticipation of selection with respect to join
A selection on a join can be moved so that the selection applies only on the single table.
That way, we reduce the size of the join.
![[selection_to_join.png]]
*F is a predicate on attributes of the $E_2$ tables

#### Anticipation of projection with respect to join
Almost the same concept of **Anticipation of selection with respect to join**.
Here, **L1** is the subset of **L** without the attributes of E2, that is **L1=L-Schema(E2)**; same thing goes for **L2**.
Moreover, **J** is the set of attributes needed to evaluate the **join predicate** $p$ 
It's used to reduce the size of the intermediate result
![[proj_to_selection.png]]

#### Cartesian into join
This is obvious since the definition of join is the application of a selection on a cartesian product.
	Here, predicate $F$ only relates attributes in both E1 and E2.
![[cartesian_to_join.png]]

#### Distribution of selection with respect to ..
- **Union:** 
  ![[distr_sel_to_union.png]]
- **Difference:**
  ![[distr_sel_to_diff.png]]

It is not possible to anticipate projection with difference
**General rule:** all binary operators are commutative and associative, **except for the difference**
#### Distribution of projection with respect to union 
It reduces the size of intermediate results.
![[distr_proj_to_join.png]]

#### Distribution of join with respect to union
![[distr_join_to_union.png]]

#### Disjunction/conjunction
![[conjuction-disjunction.png|450]]



### Cost based optimization
It's based on **data profiles** and approximate cost formulas for **access operations**

#### Data profiles
They are basically statistics about table size and data distribution among them

##### Table profiles
Quantitative information on the characteristics of tables and columns, such as:
- **cardinality** in each table
- **size in bytes** of tuples in a table
- number of **distinct values** for each attribute in a given table
- **min** and **max** values for each attribute in a given table
These stats influence decisions on DB scan or using an index to access data.
For example, we can use table profiles to estimate the size of intermediate relation expressions.

![[estimate_select.png]]
Here, we can estimate the size of a selection.
*Val* is the number of distinct values of $A_i$ in $T$.
This formula is based under the hypothesis of uniform distribution


#### Access operators
In a query tree, leaves represent physical structures such as tables, indexes etc.
Intermediates nodes are instead operations on data.

##### Sequential scan
The DBMS executes sequential access to all tuples in a table. It's also called **full table scan**.
During the scan, certain operations are performed:
- **projection**
- **selection**
- **sorting**(locally inside each block)
- **insert/update/delete**(locally inside each block)
This approach is performed when there are no other options available(no indexes available or using an index wouldn't reduce I/O cost <sub><sup>ex. most of the record in the table satisfy the selection op.</sup></sub>

##### Sorting
Classical algorithms are used to perform the sorting.
It uses the secondary memory to store intermediate results.

##### Predicate valuation
Index access can be exploited if available.
For equality predicate, an **hash**,**b+tree** or bitmap indexes are used
For range predicate, only **b+tree** can be used as it's the only useful.
Instead, for predicates with *limited* selectivity,alas most of the tuples satisfy the predicate, a full table scan is better(or even bitmap can be considered).

In case of a **conjuction of predicates**, the **most selective** predicate is evaluated first. If an index is available, it will be used to read data. Then the other predicates are evaluated on the intermediate results.
If a bitmap index is available on both predicates, the system can optimize by computing the **intersection** of **RIDs**. <sub><sup>RIDs can be pointers to memory (ex. leaf of unclustered tree)</sup></sub>

If we have a **disjunction of predicates**, indexes can be exploited only if all predicates are supported by an index because it can't produce an intermediate result by evaluation one at the time like for conjunction. If only a predicate is supported by index, it still have to do full table scan anyway.

![[bitmap_optimization.png]]
*Example of optimization based on the exploiting of bitmap indexes*
##### Join operations
Joins are critical operations, especially in a relational dbms.
There are several algorithms to perform these operations.

##### Nested loop
A single full scan is done on the outer table and fo reach tuple, a full scan on the inner table is performed. 
As the name implies, it's a double nested for loop.
It's efficient when the inner table is **small** (so that it fits in memory) and the join attribute is **indexed**.
It's **asymmetric** as it depends on the **inner table.**
![[NestedLoop.png|500]]
##### Merge scan
Both tables are firstly sorted on the join attributes.
Then they are both scanned in parallel.
It's **symmetric** however requires sorting both tables, even though they might be already sorted by previous operations and stored on disk as temporary result.

##### Hash Join
An hash function is applied on join attributes of both tables.
Tuples to be joined end up in same hash buckets.
A local sort and join is performed into each bucket.
It's a **very fast** join technique, often the chosen one
![[HashJoin.png|500]]
##### Bitmapped join index
A bitmap matrix is exploited to perform a join operations.
Basically, a bitmap matrix precomputes the join between A and B:
- one column for each **RID** in table **A**
- one row for each **RID** in table **B**
A **1** in a cell indicates that $A_x$ and $B_y$  will be joined up, **0** otherwise.
Updates may be slow and it's typically used in OLAP queries where large tables must be joined up.
![[bitmap_join.png]]
*RID 4 of table A and RID 1 of table B can be joined.*

##### Group by
Can be performed in two ways:
- **Sort based:**
	1. Firstly, sort the resulting table on the group by attributes
	2. Then compute aggregate functions on groups
- **Hash based:** 
	1. Firstly **hash functions** are applied on the group by attributes and the records are split in buckets.
	2. Next, each bucket is sorted and aggregation functions are computed
**Materialized views** can be exploited to improve performance.

### Execution plan selection
It's done by evaluating the cost of different alternatives for **reading each table** and **executing each relation operator**.
Several criterias are considered:
- The way **data** is **read** from **disk**
	- full scan, index etc
- **Execution order** aong operators
	- jon order between two join operations
- **Algorithms** used to implement each operator
	- join algorithms etc.
- When to perform sort if needed.
The optimizer builds a *tree of alternatives*.
![[tree_opt.png]]
Here, there are 4 join methods for each join order for both joins, thus $4\times 4\times 3=48$ alternatives.

Then the optimizer choses the leaf node(the blue rectangle) with the lowest cost, computed, by means of *operation research* techniques, as follows:
![[opt_cost.png]]
C<sub>I/O</sub> = cost of I/O operations
C<sub>cpu</sub> = cost of CPU operations


## Oracle optimizer 
Goal of optimizer (recap of this file):
1. Analyze the query
2. Estimate different way to execute SQL, based on cost
3. Generate a plan, step by step, on how to run the query
Point 2. is achieved by considering:
- Acces Paths
- Join orders
- Join methods
- Statistics from dictionary for tables and indexes
![[oraclePlan.png|400]]
The parsed query is represented by a set of query blocks that are nested or interrelated to each other (sub-queries); the innermost query block is optimized first (bottom-up approach).
It uses an internal **cutoff** to reduce the number of plans explored, the higher it is the more plans are explored

**Index clustering factor**: stats that is influenced by the number of blocks that need to be accessed, lower is better. If all the data with property1 are in the Block1 etc... it can avoid accessing all the blocks of a table=low factor
##### Scans
- Full table scan: **multiblock read** can be used, it is a sequential access when no idexes are avaiable
- RowId scan: index with stored the row id of my data (that is a pointer to a physical address in a certain block) but first there need to be an **index scan** (of some type). Basically it is the action that allow to transit from the pointer in a secondary index access to the actual record.
- Index scan: can be unique, range, full, fast full, bitmap.

##### Index scans
System indexes are created automatically on the primary key attributes 
SYS_#
Syntax: CREATE INDEX IndexName ON Table (Columnm,...);
DROP INDEX IndexName
###### Unique scan
To return at most a single rowid
###### Index range scans
Most used, access multiple rowid.
Ex: discipline="art" accessed with hash index can retrieve a lot of rowid, you actually know that all those records belong to that discipline but if you want to get also other attributes you need to perform a rowId scan.
###### Full scan
Index is a covering index (all information that are needed from the query)
Retrieve the data in the right order.

**Fast full scan**
Like full scan but the result is in any order

##### Statistics tables
![[statisticsOracle.png]]
Syntax: *describe table_ name* allows to view the table schema
Stats are collected automatically, you can define at what time.

Column stats may be stored as histograms which provide accurate estimates of the distribution of column data. Stored in USER/DBA_TAB_COL_STATISTICS
In height-balanced histogram If the distribution is uniform the buckets shown should have the same very similar ranges