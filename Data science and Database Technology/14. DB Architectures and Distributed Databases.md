### Client Server architectures
The simplest and most widespread architecture.
A server manages the databases while several clients access its data.
#### 2-Tier
Here, the client is called **Thick** as it also possesses some application logic to access the data
![[thicc_client.png]]
*The client is directly connected with the dbms*
#### 3-Tier
Here, the client is called **Thin** as it just sent requests(through a browser usually).
Between the client and the DBMS server, there's another server, an application one, that manages business logic.
![[thin_client.png]]
#### SQL Execution
Different modes of SQL execution:
- **Compile&go:** query is sent to the server,prepared, executed and then the result is shipped, without storing the query plan on the server. Efficient for oneshot query executions
- **Compile&store:** as above, but the query plan is stored for future usage. Efficient for repeated executions, such as parametric queries.

### Distributed architectures
Data and computation are distributed over different machines.
In this way, there's an improvement in the performance,reliability, and an increase in the availability.
Client transactions access more than one DBMS server, thus this architecture is usually way more complex compared than client-server ones.

#### Distributed DBMS 
Different DBMS servers on different networks.
Each server is **autonomous** and cooperates with other servers.
Since each server is autonomous, there's a local **autonomy**: each server manages their data in their own way. e.g: concurrency control, recovery etc.


##### Data replication
A replica of the data is stored on each network node.
There's a replication servers that manages these replicas.
It's a rather simple architecture

##### Parallel architectures
Its sole purpose is to increase performance. 
Composed of multiprocessor machines with dedicated network connections.

##### Data warehouses
Servers specialized in decision support.
Perform OLAP (On Line Analytical Processing)
##### Properties
- **Portability:** Capability of moving a program from a system to a different system. Guaranteed by the SQL standard
- **Interoperability:** Capability of different DBMS servers to cooperate on a given task. Done so by means of different interaction protocols(**ODBC** and **X-Open-DTP**)

#### Distributed Database design
One simple approach is to distribute tables: each table is stored in a different DBMS server.

##### Data Fragmentation
Another approach is to store different portions of a table in different servers.
In this way, the original table exists only in a virtual way.
This technique is 
- **complete:** each information is contained in at least one portion(fragment)
- **correct:** information in a given table can be rebuilt from its fragments.

###### Horizontal Fragmentation
Same schema of the initial table.
Each fragment is obtained by means of a selection(**WHERE SQL** $\sigma$ ) predicate on the initial table.
In this fragmentation technique, fragments are not overlapped.
![[horiz_fragmentation.png]]
*Example of a horizontal fragmentation on the Employee table. To reconstruct the original table, we union each fragment*

###### Vertical Fragmentation
Here, all tuples are included but fragmented by means of a projection(**SELECT SQL** $\pi$ ).
Vertical fragments overlap on the primary key, so that these fragments can be rebuilt onto the original table.
![[vertical_fragment.png]]
*Example of a vertical fragmentation. To reconstruct the original table, we join each fragment together:this is way we need the primary key to overlap in each fragment.*

###### Mixed fragmentation
A mixed approach between horizontal and vertical fragmentation

##### Allocation schema
It's a schema that defines how fragments are allocated throughout the different servers the Distributed DB is composed of.
If the same fragment is stored on multiple nodes, then we have a **redundant mapping**, which gives more data availability at the cost of a more complex maintenance(we need a **copy sync** mechanism).
Otherwise we have a **non redundant mapping**.

##### Transparency levels
Describe the knowledge about data(fragments) distribution of each node.
![[frag_ex.png]]
*Example used below to describe different levels of transparency*
###### Fragmentation Transparency
Here, applications know the existence of tables and not of their fragments.
![[frag_trans.png]]

###### Allocation Transparency
Applications know about fragmentations but not their allocations.
![[allo_trans.png]]
*It doesn't know if S2 resides in the same site or not.*

###### Language Transparency
Called in this way because no SQL dialects are used, but rather only SQL standard.
Here an application knows about the allocation site of each fragment.
![[lan_trans.png]]
*The programmer knows about each fragment's site*

#### Transaction Classification
Transactions can be categorized whether they can be executed on a single node(thus all the fragments needed are available on a single node) or they have to be distributed(because fragments needed for the query are in different nodes).

##### Remote
Characterized by being run on a **single remote server**.
**Remote request** if they are **read only**(only SELECT), otherwise we have **remote transaction**(any **SQL command**).

##### Distributed
- **Distributed transaction:** Each **SQL** statement(any command) is addressed to one single node, however different statements can address different servers. We need thus a **global atomicity** as each statement is like a *sub-transaction*. To address this problem, we use **2-phase commit protocol**
- **Distributed request:** Here, each **SQL** may be distributed among different servers, unlike the **distributed transaction**. Here a distributed optimization is needed.

##### Example
![[ex_distr_ex.png]]
![[ex_distr_ex2.png]]
The class of this transaction is **distributed request** since **Account** is not esplicitly partitioned in the query. 
We would have **distributed request** f instead the update instructions referenced explicitly A1 and A2.
If both update instructions referenced only A1, then we'd have **remote request**

#### Distributed DBMS Technology
**ACID** properties must be held during a transaction:
- **Atomicity:** a distributed technique is required(**2phase commit**)
- **Consistency:** contrainted are enforced locally
- **Isolation:** It requires **strict 2PL** and **2phase commit**
- **Durability:** extension of local procedures to manage atomicity in presence of failure
##### Atomicity
All nodes partecipating to a distributed transaction must implement the same decision(commit or rollback). 
A node transaction can fail due to a node failure or network failures.
This coordination between nodes is implemented with the **2 phase commit protocol**

##### 2 Phase Commit protocol
Its objective is to coordinate the conclusion of a distributed transaction.
There are two roles in this protocol:
- **Transaction Manager(TM):** It serves the purpose of a coordinator during the distributed transaction. Only one can be the TM.
- **Resource Manager(RM):** Servers which take part to the distributed transactions
Any node can be a **TM**, even the client requesting the transaction execution.

###### New 2 Phase log records
Each node have their own private logs.
###### TM log records
- **Prepare:** contains the  identity of all RMs participating to the transaction (Node ID + Process ID)
- **Global commit/abort:** final decision of the transaction to be propagated through all nodes
- **Complete:** written at the end of the protocol
###### RM log records
- **Ready:** the RM is willing to commit the transaction. Once in a ready state, the decision cannot be changed and the node has to be in a reliable state(enforcement of WAL and commit precedence rules).

###### Protocol Phase 1
In the first phase, the **global decision** is produced.
1. The **TM** writes a **prepare** record log, sends a **prepare message** to **all RM** and sets a timeout for the RM answer
2. The **RM**s wait for the **prepare message** and when they receive it:
	- If they are in a **reliable state:** write a **ready record** in the log and send a **ready message** to TM
	- If they are in a **not reliable state:** send a **not ready message** to **TM** and end the protocol
	- If the **RM** is crashed, no answer is sent
3. The TM collects all answers:
	- If **all** are **ready messages**, **commit global decision** written on the log
	- Otherwise, if even one is **not ready** or a node expired the timeout,**global abort decision** written on the log.
###### Protocl Phase 2
Here, the global decision is sent to all nodes.
1. The **TM** sends the decision to all **RMs** with a timeout expiration
2. **RMs** wait for the decision and when they receive it, commit/abort record is written on the log and the database is updated. An **ACK** message is sent to the TM
3. The TM collects all **ACKs**
	- If **all** ACK messages are received, a **complete** record is written in the log
	- If timeout **expires** and some **ACK** are still missing, a new timeout is sent and the decision is sent to the remaining **RMs**.
Repeat until all answers are received
![[2phasecommit_protocol.png]]
*Example on how the protocol behave with a TM and 1 RM.*

###### Uncertainty window
A window where the **RM** has sent the **ready/not ready** msg but has still not receive a decision. In this window, local resources are locked. Thus, this window should be as small as possible

###### Failure of RM
The warm restart procedure is modified with a new case.
If the last record in the log for transaction T is “ready”, then T does not know the global decision of its TM(if it knew, there would be a **global commit/abort** log record).

**Recovery:** A READY l list containing the IDs of all transactions in ready state.
For all transactions in the ready list, the global decision is asked to the TM at restart through a **remote recovery request**

###### Failure of TM
Different messages can be lost along the way: 
- Phase 1:**prepare(outgoing)**,**ready(incoming)** 
- Phase 2: **global decision(outcoming)**.
**Recovery:** 
- if the last record in the **TM** log is **prepare**, **global abort** decision is written in the log and propagated through all **RMs**
- if the last record in the **TM** log is **global decision**, it means the **complete** record wasn't written, so **phase 2** must be redone
###### Network Failures
Any network failures during Phase 1 causes the **global abort decision**.
Any network failures during Phase 2 causes the repetition of Phase 2.

##### X-Open-DTP
**2 Phase commit** protocol is based under the assumption that all DBMS are **homogenous**.
If we have an **heterogenous** system, we have to use **X-Open-DTP**.
Based on a client, a TM and several RM.

###### Interfaces
Several interfaces are defined for the communication:
- **TM interface:** between client and TM
- **XA interface:** between TM and RM.
DBMS servers provide **XA** interface.

###### Features
RMs are passive and only answers to remote procedure invocation from the TM.
The control of this protocol is embedded into the TM.
The protocol implements two optimizations of 2 Phase Commit and an heuristic for failures

###### Premused Abort
One of the two feature implemented.
The TM, after the restart from a failure,when no information is available in the log about the final outcome, answers abort to a remote recovery request by a RM.
In this way synchronous writes are reduced

###### Read Only
Exploited by a RM that executes a **read-only** (sub)transaction.
A **RM** answers **read-only** to a **prepare request**, doesn't write in the log and terminate the protocol.
In the **Phase 2**, the TM ignores each **RMs** that responded with **read-only**.

###### Heuristic decisions
boh?

#### Parallel DBMS
Operations can be parallelized
- **Inter-query parallelism:** **different** queries are run on different processors. Used in OLTP systems
- **Intra-query parallelism:** parts of the **same** query(thus intra) are run on different processors. Operations that can be **intra-parallelized:** Group By and Joins. Used in OLAP systems

### DBMS Benchmarks
Benchmarks used to find which DBMS best suit the needs of a company. Each benchmark is characterized by:
- Transaction load
- Database size and content
- Transaction code
- Techniques to measure performance
#### TPC-C
Simulates the behaviour of an OLTP system

#### TPC-H
Simulates the behaviour of an OLAP system

#### TPCx-HS
For big-data management(Hadoop Clusters)




